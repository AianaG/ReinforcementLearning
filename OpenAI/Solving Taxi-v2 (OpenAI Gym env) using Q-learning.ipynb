{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving OpenAI Gym Environment - (Taxi-v2)\n",
    "\n",
    "##### Using Q-learning\n",
    "\n",
    "Objective is to pick up the passenger from one position and drop them off at another in minimum possible time. \n",
    "\n",
    "There are 4 locations (R, G, Y, B) marked in the image. And the task is to pick up the passenger from one of the four locations and drop him off at other. There is a reward of +20 for a successful dropoff, and -1 for every timestep it takes and -10 for illegal pick-up and drop-off actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# command to install gym\n",
    "# !pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "current state is : 126\n"
     ]
    }
   ],
   "source": [
    "# call the environment and store it in 'env' variable\n",
    "env = gym.make(\"Taxi-v2\") # Create environment\n",
    "\n",
    "state = env.reset()\n",
    "env.render()  # helps in visualizing the environment\n",
    "\n",
    "print(\"current state is :\" ,state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Current state\n",
    "    - yellow: taxi is unoccupied\n",
    "    - green: taxi is occupied by a passenger\n",
    "    - blue: passenger\n",
    "    - magenta: destination\n",
    "    - other grids: locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State Space\n",
    "\n",
    "The state vector for this problem is (col_index, row_index, destination_locations, passenger_position)\n",
    "There are 5 rows, 5 columns, 4 destination locations and 5 passenger positions. Therefore, state space = 5x5x4x5 = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space :  500\n"
     ]
    }
   ],
   "source": [
    "# Number of possible states\n",
    "state_size = env.observation_space.n \n",
    "print(\"State space : \", state_size)\n",
    "#print(\"Current state : \" ,env.env.s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table = np.zeros((state_size, action_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(Q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining hyperparameters and episodes\n",
    "\n",
    "learning_rate = 0.1\n",
    "gamma = 0.8\n",
    "epsilon = 0.4\n",
    "\n",
    "episodes = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping the policy epsilon-greedy\n",
    "def epsilon_greedy(state, table):\n",
    "    z = np.random.random() # Randomizes a number to select whether or not to expolit\n",
    "    \n",
    "    if z > epsilon:\n",
    "        action = np.argmax(table[state])    #Exploitation: this gets the action corresponding to max q-value of current state\n",
    "    else:\n",
    "        action = env.action_space.sample()    #Exploration: randomly choosing and action\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10000\n",
      "[0.0007653377954426333, 0.05122508145920435, 1.5772921246964833e-05, 1.0805343180209093e-06, 0.0047699159593062035, 0.050756443987424404, 0.06436286067051178, 0.0990585437006537, 0.001050262494089793, 0.025772889071006055]\n",
      "Episode: 20000\n",
      "[0.024694136123894594, 0.01044143793009944, 0.0023122941511424244, 0.037949402258600884, 0.0424891128035636, 0.002881838078214294, 0, 0.016594746734374333, 4.6044022816715824e-08, 1.9881417873124363e-08]\n",
      "Episode: 30000\n",
      "[4.369837824924616e-13, 5.915268275202834e-13, 0.0004985978867608765, 0.0012500383265461323, 2.382630586161838e-07, 0.05393230513419134, 0.008778018742569671, 1.0892620139202336e-11, 3.155904426677125e-10, 0.058626139558027646]\n",
      "Episode: 40000\n",
      "[3.525511793611713e-08, 2.4424906541753444e-15, 2.7364373478278026e-07, 0, 0.06341410601032749, 3.4495338123718966e-06, 2.1562992897017352e-05, 0.0372443128442459, 3.3306690738754696e-16, 0.00028885032726511106]\n",
      "Episode: 50000\n",
      "[0, 0.009950616086616293, 5.391459187986669e-07, 0.00015280367838299558, 6.969091970177033e-12, 0.22521886387835277, 0, 0.0004156165168875958, 8.172298710906745e-05, 5.002052105851362e-09]\n",
      "Episode: 60000\n",
      "[0.009571435383179505, 1.1852345593865721e-07, 0.00021916398571786289, 0, 0, 0, 1.1546319456101628e-14, 0.00017014752065946936, 1.7314949474211971e-10, 0]\n",
      "Episode: 70000\n",
      "[0.0012702282191536796, 0, 0, 0.006970472368450942, 4.0301205395110173e-07, 1.6214355803345626e-05, 0.00040890393262049685, 3.374269522593565e-05, 0, 0]\n",
      "Episode: 80000\n",
      "[0, 0, 0.0007044270866067848, 1.0529766392153306e-08, 0.004387314614707316, 1.5819267860095465e-05, 1.3198173038908578e-06, 1.761268075384237e-06, 0, 0.0002295160711729416]\n",
      "Episode: 90000\n",
      "[1.8071868169045047e-05, 0, 1.740067965716463e-05, 0.00016688806590536842, 0, 0, 1.1546319456101628e-14, 2.597033699203166e-12, 9.167325478465216e-05, 0]\n",
      "Episode: 100000\n",
      "[0.0005441585239687186, 0, 0.00018489692921708212, 0, 1.831030882470941e-10, 0, 0, 0.002551613928400087, 0, 2.6375259150057673e-05]\n",
      "Time taken in seconds:  <module 'time' (built-in)>\n",
      "maximum difference:  2.6375259150057673e-05\n"
     ]
    }
   ],
   "source": [
    "start = time.time()    # tracking time\n",
    "deltas = []\n",
    "for episode in range(1,episodes+1):\n",
    "    \n",
    "    state = env.reset() # Reset the environment\n",
    "    done = False        # 'done' defines successfully dropping the passenger off; \n",
    "                        # resulting in an end of episode\n",
    "    Q_change = 0  # to keep a track of difference in the Q-values\n",
    "        \n",
    "    while not done:\n",
    "\n",
    "        action = epsilon_greedy(state, Q_table)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        oldQ_table = Q_table[state, action]\n",
    "                \n",
    "        # UPDATE RULE\n",
    "        Q_table[state, action] += learning_rate * (reward + gamma * \n",
    "                                                   np.max(Q_table[new_state,:])-Q_table[state,action])\n",
    "        \n",
    "        Q_change = max(Q_change, np.abs(Q_table[state][action] - oldQ_table))\n",
    "        \n",
    "        state = new_state\n",
    "                             \n",
    "    deltas.append(Q_change)\n",
    "    \n",
    "    if episode % 10000 == 0:\n",
    "        print(\"Episode: {}\".format(episode))\n",
    "        print(deltas[-10:])\n",
    "    \n",
    "#     if deltas[-1] < 0.00000001:\n",
    "#         break\n",
    "        \n",
    "    episode += 1\n",
    "\n",
    "    \n",
    "end = time.time()\n",
    "total_time = end - start\n",
    "print(\"Time taken in seconds: \", time)\n",
    "print(\"maximum difference: \", deltas[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.           0.           0.           0.           0.\n",
      "    0.        ]\n",
      " [ -1.6445568   -0.805696    -1.6445568   -0.805696     0.24288\n",
      "   -9.805696  ]\n",
      " [  0.24288      1.5536       0.24288      1.5536       3.192\n",
      "   -7.4464    ]\n",
      " ...\n",
      " [  3.19198738   5.24         3.19198981   1.5535861   -5.80799034\n",
      "   -5.80799949]\n",
      " [ -1.64457011  -0.80571269  -1.64459461  -0.805696   -10.64450087\n",
      "  -10.64411206]\n",
      " [ 11.           7.8         11.          15.           2.\n",
      "    2.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(Q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Q-learning agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's change the environment\n",
    "state = env.reset()  # reset will set the environment to a new and random state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "Episode Reward =  11\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "done = False\n",
    "cumulative_reward = 0\n",
    "\n",
    "while(done == False):\n",
    "    \n",
    "    best_action = np.argmax(Q_table[state,:]) # selecting the best action basis Q-table\n",
    "    \n",
    "    # Take the best action and observe the new state and reward\n",
    "    state, reward, done, info = env.step(best_action) \n",
    "    cumulative_reward += reward  \n",
    "    \n",
    "    time.sleep(0.5)\n",
    "    clear_output(wait=True)\n",
    "    env.render()\n",
    "    print('Episode Reward = ', cumulative_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
